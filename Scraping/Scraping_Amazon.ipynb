{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping one page of an Amazon's results page to write the items and corresponding prices onto a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_url='https://www.amazon.fr/s/ref=nb_sb_ss_i_3_3?__mk_fr_FR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&url=search-alias%3Daps&field-keywords=pot+enfant&sprefix=pot%2Caps%2C207&crid=1KJP31KJXZLPB'\n",
    "uclient = requests.get(my_url)\n",
    "#soup(uclient,\"html.parser\")\n",
    "html=bs4.BeautifulSoup(uclient.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This step basically catches all the products on he first page of results and stores them in a list called containers.\n",
    "#In the HTML code, didn't want to go too high up in tree.\n",
    "containers=html.findAll(\"div\", {\"class\":\"a-column a-span7\"})\n",
    "#The following are further offsprings of the step above. But they didnt contain all the info I wanted to scrape\n",
    "#containers2=html.findAll(\"div\", {\"class\":\"a-row\"}) \n",
    "#containers=html.findAll(\"div\",{\"aria-hidden\":\"true\"},{\"class\":\"a-column a-span12 a-text-center\"})\n",
    "len(containers)  #There are 16 products on this page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This step creates a csv file to write all the data in\n",
    "filename=\"products.csv\"\n",
    "f=open(filename,\"w\")\n",
    "headers=\"product, price\\n\"\n",
    "f.write(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating two separate lists for the products(links) and prices\n",
    "links=[]\n",
    "prices=[]\n",
    "\n",
    "\n",
    "for container in containers:\n",
    "    price1=container.find(\"span\",{\"class\":\"a-size-base a-color-price s-price a-text-bold\"})   #scraping the price\n",
    "    link=container.div.a[\"href\"]                                                              #scraping the link\n",
    "    #The following conditional is to avoid some redundant cases, where there is a false item, with certain attributes, but not others\n",
    "    if price1 is not None:\n",
    "        price2=price1.text           #to scrape the price from all the fluff around the 'span' tag\n",
    "        price2=str(price2)           #to convert unicode to string. This eliminates the 'u' prefix\n",
    "        price2=price2.replace(\",\",\".\")   #to avoid poor formatting\n",
    "        prices.append(str(price2))\n",
    "        links.append(link)\n",
    "        f.write(link +\",\"+ str(price2)+\"\\n\")       #writes the link and prices data onto the CSV file\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
